/tmp/codalab/tmpRPfwgk/run/input
/tmp/codalab/tmpRPfwgk/run/output
['scores', 'ref', 'res', 'history', 'coopetition', 'metadata']
['detailed_results.html']
Found 500 images in the folder /tmp/codalab/tmpRPfwgk/run/input/ref/Cityscapes_processed_val/Images/val

------------------------------------- Calculate Flops Results -------------------------------------
Notations:
number of parameters (Params), number of multiply-accumulate operations(MACs),
number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),
fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),
default model backpropagation takes 2.00 times as much computation as forward propagation.

Total Training Params:                                                  31.12 M 
fwd MACs:                                                               17.9097 GMACs
fwd FLOPs:                                                              35.9658 GFLOPS
fwd+bwd MACs:                                                           53.729 GMACs
fwd+bwd FLOPs:                                                          107.897 GFLOPS

-------------------------------- Detailed Calculated FLOPs Results --------------------------------
Each module caculated is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). 
 They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.

Model(
  31.12 M = 100% Params, 17.91 GMACs = 100% MACs, 35.97 GFLOPS = 49.7965% FLOPs
  (conv1): Sequential(
    2.83 K = 0.0091% Params, 717.23 MMACs = 4.0047% MACs, 1.47 GFLOPS = 1.9942% FLOPs
    (0): Conv2d(448 = 0.0014% Params, 113.25 MMACs = 0.6323% MACs, 230.69 MFLOPS = 0.3149% FLOPs, 3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32 = 0.0001% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0% FLOPs, inplace=True)
    (3): Conv2d(2.32 K = 0.0075% Params, 603.98 MMACs = 3.3724% MACs, 1.21 GFLOPS = 1.6793% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32 = 0.0001% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0% FLOPs, inplace=True)
  )
  (conv2): Sequential(
    14.02 K = 0.045% Params, 905.97 MMACs = 5.0585% MACs, 1.83 GFLOPS = 2.519% FLOPs
    (0): Conv2d(4.64 K = 0.0149% Params, 301.99 MMACs = 1.6862% MACs, 606.08 MFLOPS = 0.8397% FLOPs, 16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64 = 0.0002% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0% FLOPs, inplace=True)
    (3): Conv2d(9.25 K = 0.0297% Params, 603.98 MMACs = 3.3724% MACs, 1.21 GFLOPS = 1.6793% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(64 = 0.0002% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0% FLOPs, inplace=True)
  )
  (conv3): Sequential(
    55.68 K = 0.1789% Params, 905.97 MMACs = 5.0585% MACs, 1.82 GFLOPS = 2.519% FLOPs
    (0): Conv2d(18.5 K = 0.0594% Params, 301.99 MMACs = 1.6862% MACs, 605.03 MFLOPS = 0.8397% FLOPs, 32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128 = 0.0004% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0% FLOPs, inplace=True)
    (3): Conv2d(36.93 K = 0.1187% Params, 603.98 MMACs = 3.3724% MACs, 1.21 GFLOPS = 1.6793% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128 = 0.0004% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0% FLOPs, inplace=True)
  )
  (conv4): Sequential(
    221.95 K = 0.7132% Params, 905.97 MMACs = 5.0585% MACs, 1.82 GFLOPS = 2.519% FLOPs
    (0): Conv2d(73.86 K = 0.2373% Params, 301.99 MMACs = 1.6862% MACs, 604.5 MFLOPS = 0.8397% FLOPs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(256 = 0.0008% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0% FLOPs, inplace=True)
    (3): Conv2d(147.58 K = 0.4742% Params, 603.98 MMACs = 3.3724% MACs, 1.21 GFLOPS = 1.6793% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(256 = 0.0008% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0% FLOPs, inplace=True)
  )
  (conv5): Sequential(
    886.27 K = 2.8477% Params, 905.97 MMACs = 5.0585% MACs, 1.81 GFLOPS = 2.519% FLOPs
    (0): Conv2d(295.17 K = 0.9484% Params, 301.99 MMACs = 1.6862% MACs, 604.24 MFLOPS = 0.8397% FLOPs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(512 = 0.0016% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 262.14 KFLOPS = 0% FLOPs, inplace=True)
    (3): Conv2d(590.08 K = 1.896% Params, 603.98 MMACs = 3.3724% MACs, 1.21 GFLOPS = 1.6793% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(512 = 0.0016% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 262.14 KFLOPS = 0% FLOPs, inplace=True)
  )
  (conv6): Sequential(
    3.54 M = 11.3809% Params, 905.97 MMACs = 5.0585% MACs, 1.81 GFLOPS = 2.519% FLOPs
    (0): Conv2d(1.18 M = 3.792% Params, 301.99 MMACs = 1.6862% MACs, 604.11 MFLOPS = 0.8397% FLOPs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(1.02 K = 0.0033% Params, 0 MACs = 0% MACs, 262.14 KFLOPS = 0% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 131.07 KFLOPS = 0% FLOPs, inplace=True)
    (3): Conv2d(2.36 M = 7.5823% Params, 603.98 MMACs = 3.3724% MACs, 1.21 GFLOPS = 1.6793% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(1.02 K = 0.0033% Params, 0 MACs = 0% MACs, 262.14 KFLOPS = 0% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 131.07 KFLOPS = 0% FLOPs, inplace=True)
  )
  (max): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 8.26 MFLOPS = 0% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (bottleneck): Sequential(
    16.26 M = 52.2439% Params, 1.04 GMACs = 5.808% MACs, 2.08 GFLOPS = 2.8922% FLOPs
    (0): Conv2d(4.72 M = 15.1647% Params, 301.99 MMACs = 1.6862% MACs, 604.05 MFLOPS = 0.8397% FLOPs, 512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(2.05 K = 0.0066% Params, 0 MACs = 0% MACs, 131.07 KFLOPS = 0% FLOPs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 65.54 KFLOPS = 0% FLOPs, inplace=True)
    (3): Conv2d(9.44 M = 30.326% Params, 603.98 MMACs = 3.3724% MACs, 1.21 GFLOPS = 1.6793% FLOPs, 1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(2.05 K = 0.0066% Params, 0 MACs = 0% MACs, 131.07 KFLOPS = 0% FLOPs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 65.54 KFLOPS = 0% FLOPs, inplace=True)
    (6): ConvTranspose2d(2.1 M = 6.74% Params, 134.22 MMACs = 0.7494% MACs, 268.44 MFLOPS = 0.3732% FLOPs, 1024, 512, kernel_size=(2, 2), stride=(2, 2))
  )
  (upconv5): Sequential(
    7.61 M = 24.4373% Params, 1.95 GMACs = 10.8665% MACs, 3.89 GFLOPS = 5.4111% FLOPs
    (0): Conv2d(4.72 M = 15.163% Params, 1.21 GMACs = 6.7447% MACs, 2.42 GFLOPS = 3.3586% FLOPs, 1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(1.02 K = 0.0033% Params, 0 MACs = 0% MACs, 262.14 KFLOPS = 0% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 131.07 KFLOPS = 0% FLOPs, inplace=True)
    (3): Conv2d(2.36 M = 7.5823% Params, 603.98 MMACs = 3.3724% MACs, 1.21 GFLOPS = 1.6793% FLOPs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(1.02 K = 0.0033% Params, 0 MACs = 0% MACs, 262.14 KFLOPS = 0% FLOPs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 131.07 KFLOPS = 0% FLOPs, inplace=True)
    (6): ConvTranspose2d(524.54 K = 1.6854% Params, 134.22 MMACs = 0.7494% MACs, 268.45 MFLOPS = 0.3732% FLOPs, 512, 256, kernel_size=(2, 2), stride=(2, 2))
  )
  (upconv4): Sequential(
    1.9 M = 6.112% Params, 1.95 GMACs = 10.8665% MACs, 3.89 GFLOPS = 5.4111% FLOPs
    (0): Conv2d(1.18 M = 3.7912% Params, 1.21 GMACs = 6.7447% MACs, 2.42 GFLOPS = 3.3586% FLOPs, 512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(512 = 0.0016% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 262.14 KFLOPS = 0% FLOPs, inplace=True)
    (3): Conv2d(590.08 K = 1.896% Params, 603.98 MMACs = 3.3724% MACs, 1.21 GFLOPS = 1.6793% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(512 = 0.0016% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0% FLOPs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 262.14 KFLOPS = 0% FLOPs, inplace=True)
    (6): ConvTranspose2d(131.2 K = 0.4216% Params, 134.22 MMACs = 0.7494% MACs, 268.47 MFLOPS = 0.3732% FLOPs, 256, 128, kernel_size=(2, 2), stride=(2, 2))
  )
  (upconv3): Sequential(
    475.97 K = 1.5293% Params, 1.95 GMACs = 10.8665% MACs, 3.9 GFLOPS = 5.4111% FLOPs
    (0): Conv2d(295.04 K = 0.948% Params, 1.21 GMACs = 6.7447% MACs, 2.42 GFLOPS = 3.3586% FLOPs, 256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(256 = 0.0008% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0% FLOPs, inplace=True)
    (3): Conv2d(147.58 K = 0.4742% Params, 603.98 MMACs = 3.3724% MACs, 1.21 GFLOPS = 1.6793% FLOPs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(256 = 0.0008% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0% FLOPs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 524.29 KFLOPS = 0% FLOPs, inplace=True)
    (6): ConvTranspose2d(32.83 K = 0.1055% Params, 134.22 MMACs = 0.7494% MACs, 268.5 MFLOPS = 0.3732% FLOPs, 128, 64, kernel_size=(2, 2), stride=(2, 2))
  )
  (upconv2): Sequential(
    119.2 K = 0.383% Params, 1.95 GMACs = 10.8665% MACs, 3.9 GFLOPS = 5.4111% FLOPs
    (0): Conv2d(73.79 K = 0.2371% Params, 1.21 GMACs = 6.7447% MACs, 2.42 GFLOPS = 3.3586% FLOPs, 128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128 = 0.0004% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0% FLOPs, inplace=True)
    (3): Conv2d(36.93 K = 0.1187% Params, 603.98 MMACs = 3.3724% MACs, 1.21 GFLOPS = 1.6793% FLOPs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128 = 0.0004% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0% FLOPs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0% FLOPs, inplace=True)
    (6): ConvTranspose2d(8.22 K = 0.0264% Params, 134.22 MMACs = 0.7494% MACs, 268.57 MFLOPS = 0.3732% FLOPs, 64, 32, kernel_size=(2, 2), stride=(2, 2))
  )
  (upconv1): Sequential(
    29.9 K = 0.0961% Params, 1.95 GMACs = 10.8665% MACs, 3.91 GFLOPS = 5.4111% FLOPs
    (0): Conv2d(18.46 K = 0.0593% Params, 1.21 GMACs = 6.7447% MACs, 2.42 GFLOPS = 3.3586% FLOPs, 64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64 = 0.0002% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0% FLOPs, inplace=True)
    (3): Conv2d(9.25 K = 0.0297% Params, 603.98 MMACs = 3.3724% MACs, 1.21 GFLOPS = 1.6793% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(64 = 0.0002% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 2.1 MFLOPS = 0% FLOPs, inplace=True)
    (6): ConvTranspose2d(2.06 K = 0.0066% Params, 134.22 MMACs = 0.7494% MACs, 268.7 MFLOPS = 0.3732% FLOPs, 32, 16, kernel_size=(2, 2), stride=(2, 2))
  )
  (conv_out): Sequential(
    7.01 K = 0.0225% Params, 1.81 GMACs = 10.1171% MACs, 3.66 GFLOPS = 5.038% FLOPs
    (0): Conv2d(4.62 K = 0.0149% Params, 1.21 GMACs = 6.7447% MACs, 2.42 GFLOPS = 3.3586% FLOPs, 32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32 = 0.0001% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0% FLOPs, inplace=True)
    (3): Conv2d(2.32 K = 0.0075% Params, 603.98 MMACs = 3.3724% MACs, 1.21 GFLOPS = 1.6793% FLOPs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32 = 0.0001% Params, 0 MACs = 0% MACs, 8.39 MFLOPS = 0% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.19 MFLOPS = 0% FLOPs, inplace=True)
  )
  (output): Conv2d(323 = 0.001% Params, 79.69 MMACs = 0.445% MACs, 164.36 MFLOPS = 0.2216% FLOPs, 16, 19, kernel_size=(1, 1), stride=(1, 1))
)
---------------------------------------------------------------------------------------------------
---------
0
---------
---------
1
---------
---------
2
---------
---------
3
---------
---------
4
---------
---------
5
---------
---------
6
---------
---------
7
---------
---------
8
---------
---------
9
---------
---------
10
---------
---------
11
---------
---------
12
---------
---------
13
---------
---------
14
---------
---------
15
---------
---------
16
---------
---------
17
---------
---------
18
---------
---------
19
---------
---------
20
---------
---------
21
---------
---------
22
---------
---------
23
---------
---------
24
---------
---------
25
---------
---------
26
---------
---------
27
---------
---------
28
---------
---------
29
---------
---------
30
---------
---------
31
---------
---------
32
---------
---------
33
---------
---------
34
---------
---------
35
---------
---------
36
---------
---------
37
---------
---------
38
---------
---------
39
---------
---------
40
---------
---------
41
---------
---------
42
---------
---------
43
---------
---------
44
---------
---------
45
---------
---------
46
---------
---------
47
---------
---------
48
---------
---------
49
---------
---------
50
---------
---------
51
---------
---------
52
---------
---------
53
---------
---------
54
---------
---------
55
---------
---------
56
---------
---------
57
---------
---------
58
---------
---------
59
---------
---------
60
---------
---------
61
---------
---------
62
---------
---------
63
---------
---------
64
---------
---------
65
---------
---------
66
---------
---------
67
---------
---------
68
---------
---------
69
---------
---------
70
---------
---------
71
---------
---------
72
---------
---------
73
---------
---------
74
---------
---------
75
---------
---------
76
---------
---------
77
---------
---------
78
---------
---------
79
---------
---------
80
---------
---------
81
---------
---------
82
---------
---------
83
---------
---------
84
---------
---------
85
---------
---------
86
---------
---------
87
---------
---------
88
---------
---------
89
---------
---------
90
---------
---------
91
---------
---------
92
---------
---------
93
---------
---------
94
---------
---------
95
---------
---------
96
---------
---------
97
---------
---------
98
---------
---------
99
---------
---------
100
---------
---------
101
---------
---------
102
---------
---------
103
---------
---------
104
---------
---------
105
---------
---------
106
---------
---------
107
---------
---------
108
---------
---------
109
---------
---------
110
---------
---------
111
---------
---------
112
---------
---------
113
---------
---------
114
---------
---------
115
---------
---------
116
---------
---------
117
---------
---------
118
---------
---------
119
---------
---------
120
---------
---------
121
---------
---------
122
---------
---------
123
---------
---------
124
---------
---------
125
---------
---------
126
---------
---------
127
---------
---------
128
---------
---------
129
---------
---------
130
---------
---------
131
---------
---------
132
---------
---------
133
---------
---------
134
---------
---------
135
---------
---------
136
---------
---------
137
---------
---------
138
---------
---------
139
---------
---------
140
---------
---------
141
---------
---------
142
---------
---------
143
---------
---------
144
---------
---------
145
---------
---------
146
---------
---------
147
---------
---------
148
---------
---------
149
---------
---------
150
---------
---------
151
---------
---------
152
---------
---------
153
---------
---------
154
---------
---------
155
---------
---------
156
---------
---------
157
---------
---------
158
---------
---------
159
---------
---------
160
---------
---------
161
---------
---------
162
---------
---------
163
---------
---------
164
---------
---------
165
---------
---------
166
---------
---------
167
---------
---------
168
---------
---------
169
---------
---------
170
---------
---------
171
---------
---------
172
---------
---------
173
---------
---------
174
---------
---------
175
---------
---------
176
---------
---------
177
---------
---------
178
---------
---------
179
---------
---------
180
---------
---------
181
---------
---------
182
---------
---------
183
---------
---------
184
---------
---------
185
---------
---------
186
---------
---------
187
---------
---------
188
---------
---------
189
---------
---------
190
---------
---------
191
---------
---------
192
---------
---------
193
---------
---------
194
---------
---------
195
---------
---------
196
---------
---------
197
---------
---------
198
---------
---------
199
---------
---------
200
---------
---------
201
---------
---------
202
---------
---------
203
---------
---------
204
---------
---------
205
---------
---------
206
---------
---------
207
---------
---------
208
---------
---------
209
---------
---------
210
---------
---------
211
---------
---------
212
---------
---------
213
---------
---------
214
---------
---------
215
---------
---------
216
---------
---------
217
---------
---------
218
---------
---------
219
---------
---------
220
---------
---------
221
---------
---------
222
---------
---------
223
---------
---------
224
---------
---------
225
---------
---------
226
---------
---------
227
---------
---------
228
---------
---------
229
---------
---------
230
---------
---------
231
---------
---------
232
---------
---------
233
---------
---------
234
---------
---------
235
---------
---------
236
---------
---------
237
---------
---------
238
---------
---------
239
---------
---------
240
---------
---------
241
---------
---------
242
---------
---------
243
---------
---------
244
---------
---------
245
---------
---------
246
---------
---------
247
---------
---------
248
---------
---------
249
---------
---------
250
---------
---------
251
---------
---------
252
---------
---------
253
---------
---------
254
---------
---------
255
---------
---------
256
---------
---------
257
---------
---------
258
---------
---------
259
---------
---------
260
---------
---------
261
---------
---------
262
---------
---------
263
---------
---------
264
---------
---------
265
---------
---------
266
---------
---------
267
---------
---------
268
---------
---------
269
---------
---------
270
---------
---------
271
---------
---------
272
---------
---------
273
---------
---------
274
---------
---------
275
---------
---------
276
---------
---------
277
---------
---------
278
---------
---------
279
---------
---------
280
---------
---------
281
---------
---------
282
---------
---------
283
---------
---------
284
---------
---------
285
---------
---------
286
---------
---------
287
---------
---------
288
---------
---------
289
---------
---------
290
---------
---------
291
---------
---------
292
---------
---------
293
---------
---------
294
---------
---------
295
---------
---------
296
---------
---------
297
---------
---------
298
---------
---------
299
---------
---------
300
---------
---------
301
---------
---------
302
---------
---------
303
---------
---------
304
---------
---------
305
---------
---------
306
---------
---------
307
---------
---------
308
---------
---------
309
---------
---------
310
---------
---------
311
---------
---------
312
---------
---------
313
---------
---------
314
---------
---------
315
---------
---------
316
---------
---------
317
---------
---------
318
---------
---------
319
---------
---------
320
---------
---------
321
---------
---------
322
---------
---------
323
---------
---------
324
---------
---------
325
---------
---------
326
---------
---------
327
---------
---------
328
---------
---------
329
---------
---------
330
---------
---------
331
---------
---------
332
---------
---------
333
---------
---------
334
---------
---------
335
---------
---------
336
---------
---------
337
---------
---------
338
---------
---------
339
---------
---------
340
---------
---------
341
---------
---------
342
---------
---------
343
---------
---------
344
---------
---------
345
---------
---------
346
---------
---------
347
---------
---------
348
---------
---------
349
---------
---------
350
---------
---------
351
---------
---------
352
---------
---------
353
---------
---------
354
---------
---------
355
---------
---------
356
---------
---------
357
---------
---------
358
---------
---------
359
---------
---------
360
---------
---------
361
---------
---------
362
---------
---------
363
---------
---------
364
---------
---------
365
---------
---------
366
---------
---------
367
---------
---------
368
---------
---------
369
---------
---------
370
---------
---------
371
---------
---------
372
---------
---------
373
---------
---------
374
---------
---------
375
---------
---------
376
---------
---------
377
---------
---------
378
---------
---------
379
---------
---------
380
---------
---------
381
---------
---------
382
---------
---------
383
---------
---------
384
---------
---------
385
---------
---------
386
---------
---------
387
---------
---------
388
---------
---------
389
---------
---------
390
---------
---------
391
---------
---------
392
---------
---------
393
---------
---------
394
---------
---------
395
---------
---------
396
---------
---------
397
---------
---------
398
---------
---------
399
---------
---------
400
---------
---------
401
---------
---------
402
---------
---------
403
---------
---------
404
---------
---------
405
---------
---------
406
---------
---------
407
---------
---------
408
---------
---------
409
---------
---------
410
---------
---------
411
---------
---------
412
---------
---------
413
---------
---------
414
---------
---------
415
---------
---------
416
---------
---------
417
---------
---------
418
---------
---------
419
---------
---------
420
---------
---------
421
---------
---------
422
---------
---------
423
---------
---------
424
---------
---------
425
---------
---------
426
---------
---------
427
---------
---------
428
---------
---------
429
---------
---------
430
---------
---------
431
---------
---------
432
---------
---------
433
---------
---------
434
---------
---------
435
---------
---------
436
---------
---------
437
---------
---------
438
---------
---------
439
---------
---------
440
---------
---------
441
---------
---------
442
---------
---------
443
---------
---------
444
---------
---------
445
---------
---------
446
---------
---------
447
---------
---------
448
---------
---------
449
---------
---------
450
---------
---------
451
---------
---------
452
---------
---------
453
---------
---------
454
---------
---------
455
---------
---------
456
---------
---------
457
---------
---------
458
---------
---------
459
---------
---------
460
---------
---------
461
---------
---------
462
---------
---------
463
---------
---------
464
---------
---------
465
---------
---------
466
---------
---------
467
---------
---------
468
---------
---------
469
---------
---------
470
---------
---------
471
---------
---------
472
---------
---------
473
---------
---------
474
---------
---------
475
---------
---------
476
---------
---------
477
---------
---------
478
---------
---------
479
---------
---------
480
---------
---------
481
---------
---------
482
---------
---------
483
---------
---------
484
---------
---------
485
---------
---------
486
---------
---------
487
---------
---------
488
---------
---------
489
---------
---------
490
---------
---------
491
---------
---------
492
---------
---------
493
---------
---------
494
---------
---------
495
---------
---------
496
---------
---------
497
---------
---------
498
---------
---------
499
---------
