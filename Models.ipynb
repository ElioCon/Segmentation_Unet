{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-20T21:20:54.003865Z","iopub.status.busy":"2024-03-20T21:20:54.003240Z","iopub.status.idle":"2024-03-20T21:20:54.024634Z","shell.execute_reply":"2024-03-20T21:20:54.022968Z","shell.execute_reply.started":"2024-03-20T21:20:54.003822Z"},"trusted":true},"outputs":[],"source":["from IPython.display import clear_output\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from torchvision import datasets\n","import torchvision.transforms.v2 as transforms\n","#import torchvision.transforms as transforms\n","\n","from torchvision.datasets import Cityscapes\n","from argparse import ArgumentParser\n","\n","import torch.optim.lr_scheduler as lr_scheduler\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import gc\n","import torch.nn.functional as F\n","\n","import os\n","import random\n","\n","import pdb\n","import collections\n","from matplotlib.colors import ListedColormap\n","\n","import os\n","import albumentations as A\n","#import wandb\n","from torch.optim.lr_scheduler import StepLR\n","\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","from math import ceil\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{},"source":["### Preapre the data\n","Load the dataset and apply the transformation to the data. Split the dataset into the training and the test datasets with the desired split ratio."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Create the transform variable\n","size = 512\n","transform = transforms.Compose([\n","transforms.Resize((size, size*2), interpolation=transforms.InterpolationMode.LANCZOS),\n","transforms.ToTensor(),\n","transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), \n","])\n","\n","target_transforms = transforms.Compose([\n","    transforms.Resize((size, size*2)),\n","    transforms.ToTensor(),\n","])\n"," \n","# Load the dataset and apply transforms download=False, transform=transform, target_transform=target_transforms\n","dataset_train = datasets.Cityscapes('./data', split='train', mode='fine', target_type='semantic', transform=transform, target_transform=target_transforms)\n","\n","# Split training set into training and validation sets\n","split = 0.8\n","boundary = round(split*round(len(dataset_train)))\n","train_dataset = torch.utils.data.Subset(dataset_train, range(boundary))\n","val_dataset = torch.utils.data.Subset(dataset_train, range(boundary, len(dataset_train)))\n","\n","# Create data loaders\n","trainloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=False)\n","validationloader = DataLoader(val_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Mapping function and selection of 18 classes. \n","Please run this function in order to select 18 out of 30 classes. "]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from collections import namedtuple\n","import torch\n","\n","Label = namedtuple( 'Label' , [\n","\n","    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n","                    # We use them to uniquely name a class\n","\n","    'id'          , # An integer ID that is associated with this label.\n","                    # The IDs are used to represent the label in ground truth images\n","                    # An ID of -1 means that this label does not have an ID and thus\n","                    # is ignored when creating ground truth images (e.g. license plate).\n","                    # Do not modify these IDs, since exactly these IDs are expected by the\n","                    # evaluation server.\n","\n","    'trainId'     , # Feel free to modify these IDs as suitable for your method. Then create\n","                    # ground truth images with train IDs, using the tools provided in the\n","                    # 'preparation' folder. However, make sure to validate or submit results\n","                    # to our evaluation server using the regular IDs above!\n","                    # For trainIds, multiple labels might have the same ID. Then, these labels\n","                    # are mapped to the same class in the ground truth images. For the inverse\n","                    # mapping, we use the label that is defined first in the list below.\n","                    # For example, mapping all void-type classes to the same ID in training,\n","                    # might make sense for some approaches.\n","                    # Max value is 255!\n","\n","    'category'    , # The name of the category that this label belongs to\n","\n","    'categoryId'  , # The ID of this category. Used to create ground truth images\n","                    # on category level.\n","\n","    'hasInstances', # Whether this label distinguishes between single instances or not\n","\n","    'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n","                    # during evaluations or not\n","\n","    'color'       , # The color of this label\n","    ] )\n","\n","LABELS = [\n","    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n","    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n","    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n","    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n","    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n","    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n","    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n","    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n","    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n","    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n","    Label(  'parking'              ,  9 ,      255 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n","    Label(  'rail track'           , 10 ,      255 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n","    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n","    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n","    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n","    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n","    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n","    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n","    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n","    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n","    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n","    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n","    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n","    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n","    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n","    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n","    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n","    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n","    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n","    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n","    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n","    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n","    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n","    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n","    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n","    Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n","]\n","\n","def map_id_to_train_id(label_id):\n","    \"\"\"map the id to the train id for cityscapes masks\n","    input: Tensor of shape (batch_size, height, width) with values from 0 to 33\n","    output: Tensor of shape (batch_size, height, width) with values from 0 to 18\n","    \"\"\"\n","    # create a tensor with the same shape as the input tensor and fill it with the value 255\n","    train_id_tensor = torch.full_like(label_id, 255)\n","    for label in LABELS:\n","        # replace the value in the tensor with the train id if the value in the input tensor is equal to the id of the label\n","        train_id_tensor[label_id == label.id] = label.trainId\n","    return train_id_tensor"]},{"cell_type":"markdown","metadata":{},"source":["### Early stopping and training loop\n","This section is applicable for all the models, please run it. "]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["class EarlyStopper:\n","    def __init__(self, epoch_start=10, diff_patience=10, diff_min_delta=0.2, diff_lim = 0.35, val_patience=10, val_min_delta=0.1, val_lim = 0.3):\n","        self.epoch_start = epoch_start\n","        self.diff_patience = diff_patience\n","        self.diff_min_delta = diff_min_delta\n","        self.diff_lim = diff_lim\n","        self.val_patience = val_patience\n","        self.val_min_delta = val_min_delta\n","        self.val_lim = val_lim\n","        self.counter_val = 0\n","        self.counter_diff = 0\n","        self.min_validation_loss = float('inf')\n","\n","    def early_stop(self, epoch, running_loss, validation_loss):\n","        # Activate early stopping after #num epochs, when the validation and training errors are ~ same. \n","        if epoch > self.epoch_start:\n","            # Check if the current validation error is better, if true save it as new min_validation\n","            if validation_loss < self.min_validation_loss:\n","                self.min_validation_loss = validation_loss\n","                self.counter_val = 0\n","                # Check if validation loss is below certain threshold and stop if true\n","                if self.min_validation_loss < self.val_lim:\n","                    print(\"The validation loss is below the threshold.\\n\")\n","                    return True\n","            # Check if the validation does not improve over epochs, and stop if true\n","            elif validation_loss > (self.min_validation_loss + self.val_min_delta):\n","                self.counter_val += 1\n","                if self.counter_val >= self.val_patience:\n","                    print(\"The validation loss is not decreasing for multiple epochs.\\n\")\n","                    return True\n","                \n","            # Check if the training error is greater than validation error, if true reset    \n","            if running_loss > self.min_validation_loss:\n","                self.counter_diff = 0\n","            # Check if the training error differce from validation by certain limit, and stop if true \n","            elif running_loss < (self.min_validation_loss - self.diff_lim):\n","                print(\"The validation loss is too far away from validation error.\\n\")\n","                return True\n","            # Check if the difference betweem the training error and validation error does not decrease, and stop if true\n","            elif running_loss < (self.min_validation_loss - self.diff_min_delta):\n","                self.counter_diff += 1\n","                if self.counter_diff >= self.diff_patience:\n","                    print(\"The validation loss is far away from validation error for multiple epochs.\\n\")\n","                    return True \n","        return False\n","\n","def train_model_segmentation(model, train_loader, num_epochs=5, lr=0.01, step_size=20):\n","    criterion = nn.CrossEntropyLoss(ignore_index=255)\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    # Set scheduler to change the learning rate over number of epochs\n","    scheduler = StepLR(optimizer, step_size=step_size, gamma=0.1)\n","    # Early stopping\n","    early_stopper = EarlyStopper(epoch_start=10, diff_patience=20, diff_min_delta=1, diff_lim = 0.8, val_patience=10, val_min_delta=1, val_lim = 0.3)\n","        \n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        val_loss = 0.0\n","        for i, data in enumerate(trainloader):\n","            inputs, labels = data[0].to(device), (data[1]*255).to(device).long()\n","            labels = map_id_to_train_id(labels)#.to(device) \n","            labels=labels.squeeze(1)\n","            optimizer.zero_grad()\n","            optimizer.zero_grad()\n","\n","            outputs = model(inputs)\n","            labels = labels.squeeze(1)\n","            loss = criterion(outputs, labels)\n","            v=epoch + 1\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","            print(f'Epoch {epoch + 1}, Iteration [{i}/{len(train_loader)}], Loss: {running_loss/(i+1)}')\n","\n","        with torch.no_grad():\n","            model.eval()\n","            \n","            for i, data in enumerate(validationloader):\n","                inputs, labels = data[0].to(device), (data[1]*255).to(device).long()\n","                labels = map_id_to_train_id(labels)#.to(device)\n","                labels=labels.squeeze(1)\n","                outputs = model(inputs)\n","                \n","                loss = criterion(outputs, labels)\n","                v=epoch + 1\n","                val_loss += loss.item()\n","\n","                print(f'TEST: Epoch {epoch + 1}, Iteration [{i}/{len(validationloader)}], Loss: {val_loss/(i+1)}')\n","\n","        #visualize_segmentation(model, validationloader, device)\n","        print(f'Finished Train epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(trainloader):.4f}')\n","        print(f'Finished TEST epoch [{epoch + 1}/{num_epochs}], Loss: {val_loss / len(validationloader):.4f}')\n","        # Check if stopping is required\n","        if early_stopper.early_stop(epoch, running_loss, val_loss):             \n","            break\n","        # Change the learning rate if certain number of epochs is achieved\n","        if (epoch < 65):\n","          scheduler.step()\n","        if (v%step_size == 0):\n","          print(scheduler.get_last_lr(), \" New Learning Rate\")"]},{"cell_type":"markdown","metadata":{},"source":["## Models\n","All the following cells are optional to run. Each section contains different models: \n","- Baseline U-net model with dropout and early stopping. \n","- U-net model with the residual blocks and the attention layers. U-net with the ResNet as the backbone. \n","- U-net with the efficient blocks. U-net with EfficientNet as the backbone. "]},{"cell_type":"markdown","metadata":{},"source":["### Baseline U-net\n","This is a baseline model. The early stopping and the dropout are already added. These can be removed by modifying the following cell and the training function. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# create Segmentation model\n","class SegmentationCNN(nn.Module):\n","    def __init__(self, in_channels=3, classes=19, power=4): \n","        super(SegmentationCNN, self).__init__()\n","  \n","        # Factor of number of weights\n","        factor = 2**power\n","\n","        # Encoder (contracting path)\n","        self.conv1 = self.convolve_block(in_channels, (2**1)*factor, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = self.convolve_block((2**1)*factor, (2**2)*factor, kernel_size=3, stride=1, padding=1)\n","        self.conv3 = self.convolve_block((2**2)*factor, (2**3)*factor, kernel_size=3, stride=1, padding=1)\n","        self.conv4 = self.convolve_block((2**3)*factor, (2**4)*factor, kernel_size=3, stride=1, padding=1)\n","        self.conv5 = self.convolve_block((2**4)*factor, (2**5)*factor, kernel_size=3, stride=1, padding=1)\n","        \n","        #Max pool\n","        self.max = nn.MaxPool2d(2, stride=2)\n","        \n","        # Bottleneck\n","        self.bottleneck = self.bottleneck_block(external_channels=(2**5)*factor,internal_channels=(2**6)*factor,kernel_size=3,stride=1,padding=1)\n","    \n","        # Decoder (expanding path)\n","        self.upconv4 = self.expand_block((2**6)*factor, (2**5)*factor, kernel_size=3, stride=1, padding=1)\n","        self.upconv3 = self.expand_block((2**5)*factor, (2**4)*factor, kernel_size=3, stride=1, padding=1) \n","        self.upconv2 = self.expand_block((2**4)*factor, (2**3)*factor, kernel_size=3, stride=1, padding=1)  # Adjusting input channels\n","        self.upconv1 = self.expand_block((2**3)*factor, (2**2)*factor, kernel_size=3, stride=1, padding=1)  # Adjusting input channels\n","        \n","        # Output layer\n","        self.conv_out = self.convolve_block((2**2)*factor, (2**1)*factor, kernel_size=3, stride=1, padding=1)\n","        self.output = nn.Conv2d((2**1)*factor, classes, kernel_size=1)\n","        \n","    # Convolution block    \n","    def convolve_block(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n","        return nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout2d(p=0.5),\n","            nn.Conv2d(out_channels, out_channels, kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout2d(p=0.5),\n","        )\n","    \n","    # Bottleneck block\n","    def bottleneck_block(self, external_channels, internal_channels, kernel_size=3, stride=1, padding=1):\n","        return nn.Sequential(\n","            nn.Conv2d(external_channels, internal_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(internal_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout2d(p=0.5),\n","            nn.Conv2d(internal_channels, internal_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(internal_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout2d(p=0.5),\n","            nn.ConvTranspose2d(internal_channels, external_channels, kernel_size=2, stride=2)\n","        )\n","    \n","    # Upconvolve block\n","    def expand_block(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n","        return nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout2d(p=0.5),\n","            nn.Conv2d(out_channels, out_channels, kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout2d(p=0.5),\n","            nn.ConvTranspose2d(out_channels, out_channels=out_channels//2, kernel_size=2, stride=2),\n","        )\n","\n","    def forward(self, x):\n","        # Encoder (contracting path)\n","        conv1 = self.conv1(x)\n","        x = self.max(conv1)\n","        conv2 = self.conv2(x)\n","        x = self.max(conv2)\n","        conv3 = self.conv3(x)\n","        x = self.max(conv3)\n","        conv4 = self.conv4(x)\n","        x = self.max(conv4)\n","        conv5 = self.conv5(x)\n","        x = self.max(conv5)\n","        \n","        # Bottleneck\n","        bottleneck = self.bottleneck(x)\n","        \n","        # Decoder (expanding path)\n","        upconv4 = self.upconv4(torch.cat([conv5, bottleneck], dim=1))\n","        upconv3 = self.upconv3(torch.cat([conv4, upconv4], dim=1))\n","        upconv2 = self.upconv2(torch.cat([conv3, upconv3], dim=1))\n","        upconv1 = self.upconv1(torch.cat([conv2, upconv2], dim=1))\n","        \n","        # Output layer\n","        output = self.conv_out(torch.cat([conv1, upconv1], dim=1))\n","        output = self.output(output)\n","        return output\n","\n","# Define the model\n","model = SegmentationCNN(in_channels=3, classes=19, power=2)\n","\n","# Total parameters and trainable parameters.\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"{total_params:,} total parameters.\")\n","total_trainable_params = sum(\n","    p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"{total_trainable_params:,} training parameters.\")\n","\n","# Set the weights initialization\n","def init_weights(m):\n","    if isinstance(m, nn.Conv2d):\n","        # torch.nn.init.constant_(m.weight, 1) # Sets tensor m.weights to value of 1\n","        torch.nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))  # Uses xavier weights initialization (Var[s] = Var[x], since Var[w] = 1/n)\n","        if m.bias is not None:\n","            m.bias.data.zero_()\n","    if isinstance(m, nn.Linear):\n","        torch.nn.init.constant_(m.weight, 1) \n","        if m.bias is not None:\n","            m.bias.data.zero_()\n","\n","# Apply the weights and biases before training\n","model.apply(init_weights)\n","\n","# Set parameters\n","nb_epochs = 3\n","learning_rate = 0.01 # 0.01 for 20\n","step_size = 20\n","\n","# Train the model\n","train_model_segmentation(model, trainloader, nb_epochs, learning_rate, step_size)"]},{"cell_type":"markdown","metadata":{},"source":["\n","### U-net with the ResNet as a backbone + Attention layers\n","The main difference from the conventional U-net is that the encoder uses a ResidualBlock in order to preserve the gradient. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Sequential(\n","                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n","                        nn.BatchNorm2d(out_channels),\n","                        nn.ReLU())\n","        self.conv2 = nn.Sequential(\n","                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n","                        nn.BatchNorm2d(out_channels))\n","        self.downsample = downsample\n","        self.relu = nn.ReLU()\n","        self.out_channels = out_channels\n","        \n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.conv2(out)\n","        if self.downsample:\n","            residual = self.downsample(x)\n","        out += residual\n","        out = self.relu(out)\n","        return out\n","\n","class URes(nn.Module):\n","    def __init__(self, block = ResidualBlock, blocks = [2, 2, 2, 2], in_channels = 3, classes = 19, power = 5):\n","        super(URes, self).__init__()\n","        factor = 2**power\n","        self.inplanes = factor\n","        # First layer for edges\n","        self.conv1 = nn.Sequential(\n","                        nn.Conv2d(in_channels, self.inplanes, kernel_size = 3, stride = 1, padding = 1),\n","                        nn.BatchNorm2d(self.inplanes),\n","                        nn.ReLU())\n","        # Maxpooling \n","        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 1, padding = 1)\n","        self.max = nn.MaxPool2d(2, stride=2)\n","\n","        # Encoder\n","        self.layer0 = self._make_layer(block, self.inplanes*2, blocks[0], stride = 1)\n","        self.x_att0 = nn.Conv2d(in_channels=self.inplanes, out_channels=128, kernel_size=1, stride=2, padding=0)\n","        self.layer1 = self._make_layer(block, self.inplanes*2, blocks[1], stride = 1)\n","        self.x_att1 = nn.Conv2d(in_channels=self.inplanes, out_channels=128, kernel_size=1, stride=2, padding=0)\n","        self.layer2 = self._make_layer(block, self.inplanes*2, blocks[2], stride = 1)\n","        self.x_att2 = nn.Conv2d(in_channels=self.inplanes, out_channels=128, kernel_size=1, stride=2, padding=0)\n","        self.layer3 = self._make_layer(block, self.inplanes*2, blocks[3], stride = 1)\n","        self.x_att3 = nn.Conv2d(in_channels=self.inplanes, out_channels=128, kernel_size=1, stride=2, padding=0)\n","\n","        # Bottleneck\n","        self.bottleneck = self.bottleneck_block(external_channels=self.inplanes, internal_channels=self.inplanes*2, kernel_size=3,stride=1,padding=1)\n","        self.g_att3 = nn.Conv2d(in_channels=self.inplanes, out_channels=128, kernel_size=1, stride=1, padding=0)\n","        self.upB = nn.ConvTranspose2d(self.inplanes, self.inplanes//2, kernel_size=2, stride=2)\n","        # Decoder (expanding path)\n","        self.upconv3 = self.expand_block(self.inplanes, self.inplanes//2, kernel_size=3, stride=1, padding=1) \n","        self.g_att2 = nn.Conv2d(in_channels=self.inplanes, out_channels=128, kernel_size=1, stride=1, padding=0)\n","        self.up3 = nn.ConvTranspose2d(self.inplanes, self.inplanes//2, kernel_size=2, stride=2)\n","\n","        self.upconv2 = self.expand_block(self.inplanes, self.inplanes//2, kernel_size=3, stride=1, padding=1)  # Adjusting input channels\n","        self.g_att1 = nn.Conv2d(in_channels=self.inplanes, out_channels=128, kernel_size=1, stride=1, padding=0)\n","        self.up2 = nn.ConvTranspose2d(self.inplanes, self.inplanes//2, kernel_size=2, stride=2)\n","\n","        self.upconv1 = self.expand_block(self.inplanes, self.inplanes//2, kernel_size=3, stride=1, padding=1)  # Adjusting input channels\n","        self.g_att0 = nn.Conv2d(in_channels=self.inplanes, out_channels=128, kernel_size=1, stride=1, padding=0)\n","        self.up1 = nn.ConvTranspose2d(self.inplanes, self.inplanes//2, kernel_size=2, stride=2)\n","\n","         # Output layer\n","        self.conv_out = self.convolve_block(self.inplanes, self.inplanes//2, kernel_size=3, stride=1, padding=1)\n","        self.output = nn.Conv2d(self.inplanes, classes, kernel_size=1)\n","\n","        # Attention functions\n","        self.activateRelu = nn.ReLU(inplace=True)\n","        self.activateSig = nn.Sigmoid()\n","        self.conv_psi = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1, stride=1, padding=0)\n","        self.upconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2)\n","        \n","    def _make_layer(self, block, planes, num_blocks, stride=1):\n","        downsample = None\n","        if self.inplanes != planes:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n","                nn.BatchNorm2d(planes),\n","            )\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample))\n","        self.inplanes = planes\n","        for i in range(1, num_blocks):\n","            layers.append(block(self.inplanes, planes))\n","\n","        return nn.Sequential(*layers)\n","\n","    def convolve_block(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n","        self.inplanes = out_channels\n","        return nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","    def bottleneck_block(self, external_channels, internal_channels, kernel_size=3, stride=1, padding=1):\n","        self.inplanes = internal_channels\n","        return nn.Sequential(\n","            nn.Conv2d(external_channels, internal_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(internal_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(internal_channels, internal_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(internal_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","    \n","    def expand_block(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n","        self.inplanes = out_channels\n","        return nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","    \n","    def forward(self, x):\n","        # Encoder\n","        conv0 = self.conv1(x)       # (2**1)*power, size/(2**0)\n","        x = self.maxpool(conv0)     # size/(2**0)\n","        conv1 = self.layer0(x)      # (2**2)*power, size/(2**0)\n","        x_att0 = self.x_att0(conv1)\n","        x = self.max(conv1)         # size/(2**1)\n","        conv2 = self.layer1(x)      # (2**3)*power, size/(2**1)\n","        x_att1 = self.x_att1(conv2)\n","        x = self.max(conv2)         # size/(2**2)\n","        conv3 = self.layer2(x)      # (2**4)*power, size/(2**2)\n","        x_att2 = self.x_att2(conv3)\n","        x = self.max(conv3)         # size/(2**3)\n","        conv4 = self.layer3(x)      # (2**5)*power, size/(2**3)\n","        x_att3 = self.x_att3(conv4)\n","        x = self.max(conv4)         # size/(2**4)\n","\n","        # Bottleneck\n","        bottleneck = self.bottleneck(x) # (2**5)*power, size/(2**3)\n","        g_att3 = self.g_att3(bottleneck)\n","        bottleneck = self.upB(bottleneck)\n","        y = conv4 * self.upconv(self.activateSig(self.conv_psi(self.activateRelu(x_att3 + g_att3))))\n","\n","        upconv3 = self.upconv3(torch.cat([y, bottleneck], dim=1)) # (2**4)*power, size/(2**2)\n","        g_att2 = self.g_att2(upconv3)\n","        upconv3 = self.up3(upconv3)\n","        y = conv3 * self.upconv(self.activateSig(self.conv_psi(self.activateRelu(x_att2 + g_att2))))\n","\n","        upconv2 = self.upconv2(torch.cat([y, upconv3], dim=1)) # (2**3)*power, size/(2**1)\n","        g_att1 = self.g_att1(upconv2)\n","        upconv2 = self.up2(upconv2)\n","        y = conv2 * self.upconv(self.activateSig(self.conv_psi(self.activateRelu(x_att1 + g_att1))))\n","\n","        upconv1 = self.upconv1(torch.cat([y, upconv2], dim=1)) # (2**2)*power, size/(2**0)\n","        g_att0 = self.g_att0(upconv1)\n","        upconv1 = self.up1(upconv1)\n","        y = conv1 * self.upconv(self.activateSig(self.conv_psi(self.activateRelu(x_att0 + g_att0))))\n","\n","        # Output\n","        output = self.conv_out(torch.cat([y, upconv1], dim=1)) # (2**1)*power, size/(2**0)\n","        output = self.output(output)                               # (2**0)*power, size/(2**0)\n","\n","        return output\n","\n","model = URes(block = ResidualBlock, blocks = [2, 2, 2, 2], in_channels = 3, classes = 19, power = 3)\n","\n","# Total parameters and trainable parameters.\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"{total_params:,} total parameters.\")\n","total_trainable_params = sum(\n","    p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"{total_trainable_params:,} training parameters.\")\n","\n","# Set the weights initialization\n","def init_weights(m):\n","    if isinstance(m, nn.Conv2d):\n","        torch.nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))  # Uses xavier weights initialization (Var[s] = Var[x], since Var[w] = 1/n)\n","        if m.bias is not None:\n","            m.bias.data.zero_()\n","    if isinstance(m, nn.Linear):\n","        torch.nn.init.constant_(m.weight, 1) \n","        if m.bias is not None:\n","            m.bias.data.zero_()\n","\n","# Apply the weights and biases before training\n","model.apply(init_weights)\n","\n","# Set parameters\n","nb_epochs = 3\n","learning_rate = 0.01 # 0.01 for 20\n","step_size = 20\n","\n","# Train the model\n","train_model_segmentation(model, trainloader, nb_epochs, learning_rate, step_size)"]},{"cell_type":"markdown","metadata":{},"source":["### U-net with the EfficientNet as a backbone.\n","The main difference from the conventional U-net is that the encoder uses an MBConv block."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Model\n","class SqueezeExcitation(nn.Module):\n","    def __init__(self, input_channels, reduced_dim):\n","        super().__init__()\n","        self.se = nn.Sequential(\n","            nn.AdaptiveAvgPool2d(1),\n","            nn.Conv2d(input_channels, reduced_dim, 1),\n","            nn.SiLU(),  # SiLU activation\n","            nn.Conv2d(reduced_dim, input_channels, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return x * self.se(x)\n","\n","class MBConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, expansion_factor, stride):\n","        super().__init__()\n","        mid_channels = in_channels * expansion_factor\n","        \n","        self.use_residual = in_channels == out_channels and stride == 1\n","        self.expand_conv = nn.Conv2d(in_channels, mid_channels, 1, bias=False) if expansion_factor != 1 else nn.Identity()\n","        self.bn0 = nn.BatchNorm2d(mid_channels)\n","        self.depthwise_conv = nn.Conv2d(mid_channels, mid_channels, 3, stride, 1, groups=mid_channels, bias=False)\n","        self.bn1 = nn.BatchNorm2d(mid_channels)\n","        self.se_layer = SqueezeExcitation(mid_channels, reduced_dim=int(mid_channels / expansion_factor))\n","        self.project_conv = nn.Conv2d(mid_channels, out_channels, 1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.activation = nn.SiLU()\n","\n","    def forward(self, x):\n","        identity = x\n","        x = self.expand_conv(x)\n","        x = self.bn0(x)\n","        x = self.activation(x)\n","        x = self.depthwise_conv(x)\n","        x = self.bn1(x)\n","        x = self.activation(x)\n","        x = self.se_layer(x)\n","        x = self.project_conv(x)\n","        x = self.bn2(x)\n","        if self.use_residual:\n","            x += identity\n","        return x\n","\n","class UNetWithMBConv(nn.Module):\n","    def __init__(self, in_channels=3, num_classes=19, initial_power=5):\n","        super().__init__()\n","        self.factor = 2**initial_power\n","        # Initial block (customized for the appropriate number of input channels, e.g., 3 for RGB images)\n","        self.encoder0 = MBConv(in_channels=in_channels, out_channels=(2**0)*self.factor, expansion_factor=1, stride=1)\n","\n","        # Encoder: Increasing channels and reducing dimensions\n","        self.encoder1 = MBConv(in_channels=(2**0)*self.factor, out_channels=(2**1)*self.factor, expansion_factor=6, stride=1)\n","        self.encoder2 = MBConv(in_channels=(2**1)*self.factor, out_channels=(2**2)*self.factor, expansion_factor=6, stride=1)\n","        self.encoder3 = MBConv(in_channels=(2**2)*self.factor, out_channels=(2**3)*self.factor, expansion_factor=6, stride=1)\n","\n","        # Bottleneck\n","        self.bottleneck = self.bottleneck_block(external_channels=(2**3)*self.factor,internal_channels=(2**4)*self.factor,kernel_size=3,stride=1,padding=1)\n","\n","        # Maxpolling\n","        self.max = nn.MaxPool2d(2, stride=2)\n","\n","        # Decoder and upsample\n","        self.decoder3 = self.double_conv((2**4)*self.factor, (2**3)*self.factor)\n","        self.upconv2 = nn.ConvTranspose2d((2**3)*self.factor, (2**2)*self.factor, 2, stride=2)\n","        self.decoder2 = self.double_conv((2**3)*self.factor, (2**2)*self.factor)\n","        self.upconv1 = nn.ConvTranspose2d((2**2)*self.factor, (2**1)*self.factor, 2, stride=2)\n","        self.decoder1 = self.double_conv((2**2)*self.factor, (2**1)*self.factor)\n","        self.upconv0 = nn.ConvTranspose2d((2**1)*self.factor, (2**0)*self.factor, 2, stride=2)\n","        self.decoder0 = self.double_conv((2**1)*self.factor, (2**0)*self.factor)\n","\n","        # Final classifier\n","        self.final_conv = nn.Conv2d((2**0)*self.factor, num_classes, 1)\n","\n","    # Helper function for double convolution\n","    def double_conv(self, in_channels, out_channels):\n","        return nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","    \n","    def bottleneck_block(self, external_channels, internal_channels, kernel_size=3, stride=1, padding=1):\n","        return nn.Sequential(\n","            nn.Conv2d(external_channels, internal_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(internal_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(internal_channels, internal_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n","            nn.BatchNorm2d(internal_channels),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(internal_channels, external_channels, kernel_size=2, stride=2)\n","        )\n","\n","    def forward(self, x):\n","        # Encoder path\n","        enc0 = self.encoder0(x)     # size//(2**0)\n","        x = self.max(enc0)          # size//(2**1)\n","        enc1 = self.encoder1(x)     # size//(2**1)\n","        x = self.max(enc1)          # size//(2**2)         \n","        enc2 = self.encoder2(x)     # size//(2**2)\n","        x = self.max(enc2)          # size//(2**3)\n","        enc3 = self.encoder3(x)     # size//(2**3)\n","        x = self.max(enc3)          # size//(2**4)\n","\n","        # Bottelneck\n","        bottelneck = self.bottleneck(x) # size//(2**3)\n","\n","        # Decoder path\n","        dec3 = self.decoder3(torch.cat([bottelneck, enc3], dim=1)) # size//(2**3)\n","        dec2 = self.upconv2(dec3)           # size//(2**2) \n","        dec2 = self.decoder2(torch.cat([dec2, enc2], dim=1))  # size//(2**2)\n","        dec1 = self.upconv1(dec2)           # size//(2**1)\n","        dec1 = self.decoder1(torch.cat([dec1, enc1], dim=1))  # size//(2**1)\n","        dec0 = self.upconv0(dec1)           # size//(2**0)\n","        dec0 = self.decoder0(torch.cat([dec0, enc0], dim=1))  # size//(2**0)\n","\n","        # Output layer\n","        out = self.final_conv(dec0)\n","        return out\n","    \n","def init_weights(m):\n","    if isinstance(m, nn.Conv2d):\n","        torch.nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))  # Uses xavier weights initialization (Var[s] = Var[x], since Var[w] = 1/n)\n","        if m.bias is not None:\n","            m.bias.data.zero_()\n","    if isinstance(m, nn.Linear):\n","        torch.nn.init.constant_(m.weight, 1) \n","        if m.bias is not None:\n","            m.bias.data.zero_()\n","\n","model = UNetWithMBConv(in_channels=3, num_classes=19, initial_power=3) \n","# Total parameters and trainable parameters.\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"{total_params:,} total parameters.\")\n","total_trainable_params = sum(\n","    p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"{total_trainable_params:,} training parameters.\")\n","\n","# Apply the weights and biases before training\n","model.apply(init_weights)\n","\n","# Plot the distribution of the weights\n","nb_epochs = 3\n","learning_rate = 0.01 # 0.01 for 20\n","step_size = 20\n","\n","# Train the model\n","train_model_segmentation(model, trainloader, nb_epochs, learning_rate, step_size)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4515724,"sourceId":7841762,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":4}
